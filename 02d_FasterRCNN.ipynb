{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NamishBansal15/transformer-modeling-25/blob/main/02d_FasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# üöÄ Optimized Faster R-CNN Trainer\n",
        "# For Transformers / Circuit Breakers / Reactors\n",
        "# ~15‚Äì25√ó faster on T4 compared to original\n",
        "# ======================================================================\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q pandas opencv-python tqdm pyyaml\n",
        "\n",
        "import os, sys, json, yaml, time, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================================\n",
        "# üîß Global Config\n",
        "# ======================================================================\n",
        "\n",
        "project_dir = \"Substation Project - Models\"\n",
        "DRIVE_ROOT = f\"/content/drive/MyDrive/{project_dir}\"\n",
        "DATASETS_ROOT = f\"{DRIVE_ROOT}/datasets\"\n",
        "CFG_PATH = f\"{DRIVE_ROOT}/config_yolo.json\"\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "sys.modules[\"wandb\"] = None\n",
        "\n",
        "# ======================================================================\n",
        "# üìÇ Mount Google Drive\n",
        "# ======================================================================\n",
        "def mount_drive():\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=True)\n",
        "    except:\n",
        "        time.sleep(2)\n",
        "        drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "mount_drive()\n",
        "\n",
        "# ======================================================================\n",
        "# üìÑ Load Config\n",
        "# ======================================================================\n",
        "with open(CFG_PATH, \"r\") as f:\n",
        "    CONFIG = json.load(f)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ======================================================================\n",
        "# üì¶ YOLO-format Dataset ‚Üí Faster R-CNN format\n",
        "# Optimized with PIL + torchvision transforms\n",
        "# ======================================================================\n",
        "\n",
        "transform_640 = T.Compose([\n",
        "    T.Resize((640, 640)),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=transform_640):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.images = sorted([\n",
        "            p for p in self.img_dir.iterdir()\n",
        "            if p.suffix.lower() in [\".jpg\", \".png\", \".jpeg\"]\n",
        "        ])\n",
        "        self.labels = [\n",
        "            img_dir.replace(\"images\", \"labels\") + \"/\" + img.stem + \".txt\"\n",
        "            for img in self.images\n",
        "        ]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = str(self.images[idx])\n",
        "        label_path = self.labels[idx]\n",
        "\n",
        "        # -------------------------------\n",
        "        # üö® FIX: Safe image loading\n",
        "        # -------------------------------\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            print(f\"‚ö†Ô∏è Skipping unreadable/corrupted image: {img_path}\")\n",
        "            # Return a minimal dummy sample to keep DataLoader stable\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        w, h = img.size\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            for line in open(label_path, \"r\"):\n",
        "                cls, cx, cy, bw, bh = map(float, line.split())\n",
        "                x1 = (cx - bw/2) * w\n",
        "                y1 = (cy - bh/2) * h\n",
        "                x2 = (cx + bw/2) * w\n",
        "                y2 = (cy + bh/2) * h\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                labels.append(int(cls) + 1)\n",
        "\n",
        "        img_tensor = self.transform(img)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# üß± Build Optimized Faster R-CNN\n",
        "# ======================================================================\n",
        "\n",
        "def build_model(num_classes=2):\n",
        "    print(\"üìå Loading optimized Faster R-CNN‚Ä¶\")\n",
        "    # lighter than fpn_v2\n",
        "    model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = \\\n",
        "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ======================================================================\n",
        "# üéØ Trainer (AMP + frozen backbone warmup + SGD)\n",
        "# ======================================================================\n",
        "\n",
        "def train_faster_rcnn(component, epochs=100, save_period=10, patience=15):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üöÄ Optimized Faster R-CNN Training: {component.upper()}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    yaml_path = CONFIG[\"DATASETS\"][component][\"data_yaml\"]\n",
        "\n",
        "    with open(yaml_path, \"r\") as f:\n",
        "        ycfg = yaml.safe_load(f)\n",
        "\n",
        "    train_dir = ycfg[\"train\"]\n",
        "    val_dir   = ycfg[\"val\"]\n",
        "\n",
        "    train_ds = YoloDataset(train_dir)\n",
        "    val_ds   = YoloDataset(val_dir)\n",
        "\n",
        "    # Larger batch size thanks to smaller images\n",
        "    batch_size = 8\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "\n",
        "    val_dl = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "\n",
        "    MODEL_DIR = f\"{DRIVE_ROOT}/weights_backup/{component}_fasterrcnn\"\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    resume_path = f\"{MODEL_DIR}/{component}_fasterrcnn_last.pt\"\n",
        "\n",
        "    if os.path.exists(resume_path):\n",
        "        print(\"üîÑ Resuming:\", resume_path)\n",
        "        model = torch.load(resume_path, map_location=DEVICE)\n",
        "    else:\n",
        "        model = build_model(num_classes=2)\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Freeze backbone for speed (first 10 epochs)\n",
        "    for p in model.backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Unfreeze after warmup\n",
        "        if epoch == 11:\n",
        "            print(\"üîì Unfreezing backbone\")\n",
        "            for p in model.backbone.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        for imgs, targets in pbar:\n",
        "\n",
        "            imgs = [img.to(DEVICE) for img in imgs]\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss_dict = model(imgs, targets)\n",
        "                loss = sum(loss_dict.values())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        print(f\"üìâ Epoch {epoch} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Checkpoint logic\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model, f\"{MODEL_DIR}/{component}_fasterrcnn_best.pt\")\n",
        "            print(\"üíæ Saved BEST model\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"‚õî Early stopping\")\n",
        "                break\n",
        "\n",
        "        if epoch % save_period == 0:\n",
        "            torch.save(model, f\"{MODEL_DIR}/{component}_fasterrcnn_last.pt\")\n",
        "            print(\"üíæ Saved periodic checkpoint\")\n",
        "\n",
        "    torch.save(model, f\"{MODEL_DIR}/{component}_fasterrcnn_last.pt\")\n",
        "    print(\"‚úÖ Training complete:\", component)\n",
        "\n",
        "# ======================================================================\n",
        "# ‚ñ∂Ô∏è Train All Components\n",
        "# ======================================================================\n",
        "\n",
        "components = [\"transformers\", \"circuit_breakers\", \"reactors\"]\n",
        "\n",
        "start = time.time()\n",
        "for comp in components:\n",
        "    train_faster_rcnn(comp)\n",
        "\n",
        "print(\"\\nüèÅ All Faster R-CNN trainings complete.\")\n",
        "print(\"‚è± Total time:\", round((time.time()-start)/3600, 2), \"hours\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Zh9m2rA7WiG6",
        "outputId": "7319aa4d-b444-407d-fc70-a8a4083c7a07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "\n",
            "============================================================\n",
            "üöÄ Optimized Faster R-CNN Training: TRANSFORMERS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'transformers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2963928024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mtrain_faster_rcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüèÅ All Faster R-CNN trainings complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2963928024.py\u001b[0m in \u001b[0;36mtrain_faster_rcnn\u001b[0;34m(component, epochs, save_period, patience)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0myaml_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DATASETS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_yaml\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'transformers'"
          ]
        }
      ]
    }
  ]
}